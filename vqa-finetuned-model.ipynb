{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:12:43.222537Z",
     "iopub.status.busy": "2024-05-10T18:12:43.222198Z",
     "iopub.status.idle": "2024-05-10T18:13:10.565965Z",
     "shell.execute_reply": "2024-05-10T18:13:10.565055Z",
     "shell.execute_reply.started": "2024-05-10T18:12:43.222510Z"
    },
    "id": "LXCtXJ1WTtJH",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:13:45.735352Z",
     "iopub.status.busy": "2024-05-10T18:13:45.734962Z",
     "iopub.status.idle": "2024-05-10T18:13:58.494033Z",
     "shell.execute_reply": "2024-05-10T18:13:58.493094Z",
     "shell.execute_reply.started": "2024-05-10T18:13:45.735319Z"
    },
    "id": "7LIqrrVUTuJC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "import requests\n",
    "from PIL import Image\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T18:13:58.496009Z",
     "iopub.status.busy": "2024-05-10T18:13:58.495561Z",
     "iopub.status.idle": "2024-05-10T18:13:58.558010Z",
     "shell.execute_reply": "2024-05-10T18:13:58.556937Z",
     "shell.execute_reply.started": "2024-05-10T18:13:58.495983Z"
    },
    "id": "uXssIJhKmiN8",
    "outputId": "7e0a8fa7-ba8c-4c24-85e5-a41c5cf6bd64",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device and move model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:16:04.717134Z",
     "iopub.status.busy": "2024-05-10T18:16:04.716525Z",
     "iopub.status.idle": "2024-05-10T18:16:04.726247Z",
     "shell.execute_reply": "2024-05-10T18:16:04.725267Z",
     "shell.execute_reply.started": "2024-05-10T18:16:04.717101Z"
    },
    "id": "aLv5G307T0DD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, processor,max_length):\n",
    "        # Load your data from the provided path (replace with your logic)\n",
    "        self.data = data\n",
    "        self.processor = processor\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        image = self.data['image'][idx]\n",
    "        questions = self.data['question'][idx]\n",
    "\n",
    "        encoding = self.processor(image, questions, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        # remove batch dimension\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "        # add labels\n",
    "        labels = self.data['answer'][idx]\n",
    "        scores = self.data['weight'][idx]\n",
    "        targets = torch.zeros(len(id2label))\n",
    "        targets[labels] = scores\n",
    "        encoding[\"labels\"] = targets\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:17:20.235577Z",
     "iopub.status.busy": "2024-05-10T18:17:20.235201Z",
     "iopub.status.idle": "2024-05-10T18:17:36.613417Z",
     "shell.execute_reply": "2024-05-10T18:17:36.612659Z",
     "shell.execute_reply.started": "2024-05-10T18:17:20.235549Z"
    },
    "id": "dt8ymApdT2Q8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-10 18:17:24.219525: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-10 18:17:24.219650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-10 18:17:24.471035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09c7db114d5e4f68bb5f6a4169eed0d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6146d8d3ab114983add1fc81623c2bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423b51d7e996412a8f52a1f0476c9b06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769cb8006880458493bd809430bd5186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba84bb2182a48f380d0379b8afa033e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T19:04:10.715760Z",
     "iopub.status.busy": "2024-05-10T19:04:10.714988Z",
     "iopub.status.idle": "2024-05-10T19:04:19.191898Z",
     "shell.execute_reply": "2024-05-10T19:04:19.191109Z",
     "shell.execute_reply.started": "2024-05-10T19:04:10.715720Z"
    },
    "id": "8BlIFjCrdbcA",
    "outputId": "8aae2150-53d5-4573-f0cf-b5b28b080d7e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"flaviagiammarino/vqa-rad\")['train'][:]\n",
    "dataset['weight'] = torch.ones(len(dataset['image']))\n",
    "\n",
    "\n",
    "def replace_str(inputs):\n",
    "    return label2id[inputs]\n",
    "\n",
    "def replace_ids(inputs):\n",
    "    return id2label[inputs]\n",
    "\n",
    "labels = dataset['answer']\n",
    "\n",
    "unique_labels = []\n",
    "\n",
    "for label in labels:\n",
    "    if label not in unique_labels:\n",
    "        unique_labels.append(label)\n",
    "\n",
    "\n",
    "\n",
    "label2id = {\"<S>\":0,\"<E>\":1}\n",
    "num_id = 2\n",
    "for label in unique_labels:\n",
    "    label2id[label] = num_id\n",
    "    num_id+=1\n",
    "\n",
    "label2id\n",
    "\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "\n",
    "answers = []\n",
    "for data in dataset['answer']:\n",
    "    answers.append(replace_str(data))\n",
    "\n",
    "dataset['answer'] = answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T19:05:02.219965Z",
     "iopub.status.busy": "2024-05-10T19:05:02.219621Z",
     "iopub.status.idle": "2024-05-10T19:05:02.226030Z",
     "shell.execute_reply": "2024-05-10T19:05:02.225132Z",
     "shell.execute_reply.started": "2024-05-10T19:05:02.219940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'non-enhanced'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label[101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T19:04:41.163933Z",
     "iopub.status.busy": "2024-05-10T19:04:41.163009Z",
     "iopub.status.idle": "2024-05-10T19:04:41.169505Z",
     "shell.execute_reply": "2024-05-10T19:04:41.168489Z",
     "shell.execute_reply.started": "2024-05-10T19:04:41.163895Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T19:04:41.975625Z",
     "iopub.status.busy": "2024-05-10T19:04:41.975195Z",
     "iopub.status.idle": "2024-05-10T19:04:42.978060Z",
     "shell.execute_reply": "2024-05-10T19:04:42.977072Z",
     "shell.execute_reply.started": "2024-05-10T19:04:41.975597Z"
    },
    "id": "Tykxgjx8pDlb",
    "outputId": "f2a1e061-dd5e-4448-f34d-ce34c50ce8d8",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ViltForQuestionAnswering(\n",
       "  (vilt): ViltModel(\n",
       "    (embeddings): ViltEmbeddings(\n",
       "      (text_embeddings): TextEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768)\n",
       "        (position_embeddings): Embedding(40, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (patch_embeddings): ViltPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n",
       "      )\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViltEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViltLayer(\n",
       "          (attention): ViltAttention(\n",
       "            (attention): ViltSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViltSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViltIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViltOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViltPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=1536, bias=True)\n",
       "    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Linear(in_features=1536, out_features=434, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",id2label=id2label, label2id=label2id)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if not name.startswith(\"classifier\"):\n",
    "        param.requires_grad = False\n",
    "\n",
    "model.classifier = torch.nn.Sequential(\n",
    "    nn.Linear(in_features=768, out_features=1536, bias=True),\n",
    "  nn.LayerNorm((1536,), eps=1e-05, elementwise_affine=True),\n",
    "  nn.GELU(approximate='none'),\n",
    "  nn.Linear(in_features=1536, out_features=len(id2label), bias=True),\n",
    ")\n",
    "\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T19:04:42.979730Z",
     "iopub.status.busy": "2024-05-10T19:04:42.979437Z",
     "iopub.status.idle": "2024-05-10T19:04:42.988105Z",
     "shell.execute_reply": "2024-05-10T19:04:42.987277Z",
     "shell.execute_reply.started": "2024-05-10T19:04:42.979705Z"
    },
    "id": "WmrusPZ1Wm0G",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    pixel_values = [item['pixel_values'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    token_type_ids = [item['token_type_ids']for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "\n",
    "    my_dict = {'input_ids':input_ids,'attention_mask':attention_mask}\n",
    "\n",
    "    image_encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n",
    "    text_encoding = processor.tokenizer.pad(my_dict,padding = 'max_length',return_tensors = \"pt\")\n",
    "\n",
    "    batch = {}\n",
    "    batch['input_ids'] = text_encoding['input_ids']\n",
    "    batch['attention_mask'] = text_encoding['attention_mask']\n",
    "    batch['pixel_values'] = image_encoding['pixel_values']\n",
    "    batch['pixel_mask'] = image_encoding['pixel_mask']\n",
    "    batch['labels'] = torch.stack(labels)\n",
    "\n",
    "    return batch\n",
    "\n",
    "\n",
    "train_dataset = VQADataset(dataset, processor,max_length = max)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, collate_fn = collate_fn, batch_size=4,shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T19:05:19.884544Z",
     "iopub.status.busy": "2024-05-10T19:05:19.883827Z",
     "iopub.status.idle": "2024-05-10T19:05:19.971095Z",
     "shell.execute_reply": "2024-05-10T19:05:19.970073Z",
     "shell.execute_reply.started": "2024-05-10T19:05:19.884514Z"
    },
    "id": "_iNl2hwNipnm",
    "outputId": "cf548b9c-6969-498f-82ac-ea10e4b50a95",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_loader):\n",
    "    print(batch['labels'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T19:06:41.826923Z",
     "iopub.status.busy": "2024-05-10T19:06:41.826559Z",
     "iopub.status.idle": "2024-05-10T19:07:22.639337Z",
     "shell.execute_reply": "2024-05-10T19:07:22.638380Z",
     "shell.execute_reply.started": "2024-05-10T19:06:41.826893Z"
    },
    "id": "eraU_J2xipi4",
    "outputId": "0968a616-3e9a-40c6-8260-9ce7316ce122",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/300, Step: 1/4, Loss: 16.93903350830078\n",
      "Epoch: 2/300, Step: 1/4, Loss: 16.242759704589844\n",
      "Epoch: 3/300, Step: 1/4, Loss: 15.597319602966309\n",
      "Epoch: 4/300, Step: 1/4, Loss: 14.989378929138184\n",
      "Epoch: 5/300, Step: 1/4, Loss: 14.416669845581055\n",
      "Epoch: 6/300, Step: 1/4, Loss: 13.875\n",
      "Epoch: 7/300, Step: 1/4, Loss: 13.360313415527344\n",
      "Epoch: 8/300, Step: 1/4, Loss: 12.86999225616455\n",
      "Epoch: 9/300, Step: 1/4, Loss: 12.402443885803223\n",
      "Epoch: 10/300, Step: 1/4, Loss: 11.956439971923828\n",
      "Epoch: 11/300, Step: 1/4, Loss: 11.530654907226562\n",
      "Epoch: 12/300, Step: 1/4, Loss: 11.12398910522461\n",
      "Epoch: 13/300, Step: 1/4, Loss: 10.73564624786377\n",
      "Epoch: 14/300, Step: 1/4, Loss: 10.364906311035156\n",
      "Epoch: 15/300, Step: 1/4, Loss: 10.011009216308594\n",
      "Epoch: 16/300, Step: 1/4, Loss: 9.673189163208008\n",
      "Epoch: 17/300, Step: 1/4, Loss: 9.350709915161133\n",
      "Epoch: 18/300, Step: 1/4, Loss: 9.042869567871094\n",
      "Epoch: 19/300, Step: 1/4, Loss: 8.748971939086914\n",
      "Epoch: 20/300, Step: 1/4, Loss: 8.468330383300781\n",
      "Epoch: 21/300, Step: 1/4, Loss: 8.200295448303223\n",
      "Epoch: 22/300, Step: 1/4, Loss: 7.944252967834473\n",
      "Epoch: 23/300, Step: 1/4, Loss: 7.699598789215088\n",
      "Epoch: 24/300, Step: 1/4, Loss: 7.465758323669434\n",
      "Epoch: 25/300, Step: 1/4, Loss: 7.242187023162842\n",
      "Epoch: 26/300, Step: 1/4, Loss: 7.028370380401611\n",
      "Epoch: 27/300, Step: 1/4, Loss: 6.823818206787109\n",
      "Epoch: 28/300, Step: 1/4, Loss: 6.628042221069336\n",
      "Epoch: 29/300, Step: 1/4, Loss: 6.440584659576416\n",
      "Epoch: 30/300, Step: 1/4, Loss: 6.261001110076904\n",
      "Epoch: 31/300, Step: 1/4, Loss: 6.088878631591797\n",
      "Epoch: 32/300, Step: 1/4, Loss: 5.923818588256836\n",
      "Epoch: 33/300, Step: 1/4, Loss: 5.765441417694092\n",
      "Epoch: 34/300, Step: 1/4, Loss: 5.613379955291748\n",
      "Epoch: 35/300, Step: 1/4, Loss: 5.467298984527588\n",
      "Epoch: 36/300, Step: 1/4, Loss: 5.326879024505615\n",
      "Epoch: 37/300, Step: 1/4, Loss: 5.191822528839111\n",
      "Epoch: 38/300, Step: 1/4, Loss: 5.0618486404418945\n",
      "Epoch: 39/300, Step: 1/4, Loss: 4.9366936683654785\n",
      "Epoch: 40/300, Step: 1/4, Loss: 4.816119194030762\n",
      "Epoch: 41/300, Step: 1/4, Loss: 4.69989013671875\n",
      "Epoch: 42/300, Step: 1/4, Loss: 4.587800979614258\n",
      "Epoch: 43/300, Step: 1/4, Loss: 4.4796552658081055\n",
      "Epoch: 44/300, Step: 1/4, Loss: 4.375265121459961\n",
      "Epoch: 45/300, Step: 1/4, Loss: 4.274462699890137\n",
      "Epoch: 46/300, Step: 1/4, Loss: 4.177082061767578\n",
      "Epoch: 47/300, Step: 1/4, Loss: 4.082976818084717\n",
      "Epoch: 48/300, Step: 1/4, Loss: 3.9920082092285156\n",
      "Epoch: 49/300, Step: 1/4, Loss: 3.9040446281433105\n",
      "Epoch: 50/300, Step: 1/4, Loss: 3.818969964981079\n",
      "Epoch: 51/300, Step: 1/4, Loss: 3.736673355102539\n",
      "Epoch: 52/300, Step: 1/4, Loss: 3.657052755355835\n",
      "Epoch: 53/300, Step: 1/4, Loss: 3.580012083053589\n",
      "Epoch: 54/300, Step: 1/4, Loss: 3.505465030670166\n",
      "Epoch: 55/300, Step: 1/4, Loss: 3.433326005935669\n",
      "Epoch: 56/300, Step: 1/4, Loss: 3.3635177612304688\n",
      "Epoch: 57/300, Step: 1/4, Loss: 3.295964479446411\n",
      "Epoch: 58/300, Step: 1/4, Loss: 3.2305941581726074\n",
      "Epoch: 59/300, Step: 1/4, Loss: 3.167332649230957\n",
      "Epoch: 60/300, Step: 1/4, Loss: 3.1061131954193115\n",
      "Epoch: 61/300, Step: 1/4, Loss: 3.046865940093994\n",
      "Epoch: 62/300, Step: 1/4, Loss: 2.9895219802856445\n",
      "Epoch: 63/300, Step: 1/4, Loss: 2.9340128898620605\n",
      "Epoch: 64/300, Step: 1/4, Loss: 2.8802714347839355\n",
      "Epoch: 65/300, Step: 1/4, Loss: 2.8282313346862793\n",
      "Epoch: 66/300, Step: 1/4, Loss: 2.777827262878418\n",
      "Epoch: 67/300, Step: 1/4, Loss: 2.7289865016937256\n",
      "Epoch: 68/300, Step: 1/4, Loss: 2.681645393371582\n",
      "Epoch: 69/300, Step: 1/4, Loss: 2.635737419128418\n",
      "Epoch: 70/300, Step: 1/4, Loss: 2.591196060180664\n",
      "Epoch: 71/300, Step: 1/4, Loss: 2.547959327697754\n",
      "Epoch: 72/300, Step: 1/4, Loss: 2.5059657096862793\n",
      "Epoch: 73/300, Step: 1/4, Loss: 2.4651572704315186\n",
      "Epoch: 74/300, Step: 1/4, Loss: 2.425478458404541\n",
      "Epoch: 75/300, Step: 1/4, Loss: 2.386878490447998\n",
      "Epoch: 76/300, Step: 1/4, Loss: 2.3493099212646484\n",
      "Epoch: 77/300, Step: 1/4, Loss: 2.3127269744873047\n",
      "Epoch: 78/300, Step: 1/4, Loss: 2.2770886421203613\n",
      "Epoch: 79/300, Step: 1/4, Loss: 2.2423555850982666\n",
      "Epoch: 80/300, Step: 1/4, Loss: 2.2084951400756836\n",
      "Epoch: 81/300, Step: 1/4, Loss: 2.1754722595214844\n",
      "Epoch: 82/300, Step: 1/4, Loss: 2.143259286880493\n",
      "Epoch: 83/300, Step: 1/4, Loss: 2.1118276119232178\n",
      "Epoch: 84/300, Step: 1/4, Loss: 2.0811524391174316\n",
      "Epoch: 85/300, Step: 1/4, Loss: 2.051208257675171\n",
      "Epoch: 86/300, Step: 1/4, Loss: 2.021972417831421\n",
      "Epoch: 87/300, Step: 1/4, Loss: 1.9934204816818237\n",
      "Epoch: 88/300, Step: 1/4, Loss: 1.965531587600708\n",
      "Epoch: 89/300, Step: 1/4, Loss: 1.9382882118225098\n",
      "Epoch: 90/300, Step: 1/4, Loss: 1.9116662740707397\n",
      "Epoch: 91/300, Step: 1/4, Loss: 1.8856474161148071\n",
      "Epoch: 92/300, Step: 1/4, Loss: 1.8602123260498047\n",
      "Epoch: 93/300, Step: 1/4, Loss: 1.8353434801101685\n",
      "Epoch: 94/300, Step: 1/4, Loss: 1.8110222816467285\n",
      "Epoch: 95/300, Step: 1/4, Loss: 1.7872319221496582\n",
      "Epoch: 96/300, Step: 1/4, Loss: 1.7639563083648682\n",
      "Epoch: 97/300, Step: 1/4, Loss: 1.741176962852478\n",
      "Epoch: 98/300, Step: 1/4, Loss: 1.718881607055664\n",
      "Epoch: 99/300, Step: 1/4, Loss: 1.6970523595809937\n",
      "Epoch: 100/300, Step: 1/4, Loss: 1.6756762266159058\n",
      "Epoch: 101/300, Step: 1/4, Loss: 1.654740810394287\n",
      "Epoch: 102/300, Step: 1/4, Loss: 1.6342302560806274\n",
      "Epoch: 103/300, Step: 1/4, Loss: 1.6141341924667358\n",
      "Epoch: 104/300, Step: 1/4, Loss: 1.5944397449493408\n",
      "Epoch: 105/300, Step: 1/4, Loss: 1.575135588645935\n",
      "Epoch: 106/300, Step: 1/4, Loss: 1.5562105178833008\n",
      "Epoch: 107/300, Step: 1/4, Loss: 1.5376548767089844\n",
      "Epoch: 108/300, Step: 1/4, Loss: 1.5194581747055054\n",
      "Epoch: 109/300, Step: 1/4, Loss: 1.5016075372695923\n",
      "Epoch: 110/300, Step: 1/4, Loss: 1.4840996265411377\n",
      "Epoch: 111/300, Step: 1/4, Loss: 1.4669201374053955\n",
      "Epoch: 112/300, Step: 1/4, Loss: 1.4500619173049927\n",
      "Epoch: 113/300, Step: 1/4, Loss: 1.4335176944732666\n",
      "Epoch: 114/300, Step: 1/4, Loss: 1.4172773361206055\n",
      "Epoch: 115/300, Step: 1/4, Loss: 1.4013347625732422\n",
      "Epoch: 116/300, Step: 1/4, Loss: 1.3856815099716187\n",
      "Epoch: 117/300, Step: 1/4, Loss: 1.3703099489212036\n",
      "Epoch: 118/300, Step: 1/4, Loss: 1.355212926864624\n",
      "Epoch: 119/300, Step: 1/4, Loss: 1.3403847217559814\n",
      "Epoch: 120/300, Step: 1/4, Loss: 1.3258161544799805\n",
      "Epoch: 121/300, Step: 1/4, Loss: 1.3115034103393555\n",
      "Epoch: 122/300, Step: 1/4, Loss: 1.2974393367767334\n",
      "Epoch: 123/300, Step: 1/4, Loss: 1.283616304397583\n",
      "Epoch: 124/300, Step: 1/4, Loss: 1.2700304985046387\n",
      "Epoch: 125/300, Step: 1/4, Loss: 1.256676435470581\n",
      "Epoch: 126/300, Step: 1/4, Loss: 1.2435446977615356\n",
      "Epoch: 127/300, Step: 1/4, Loss: 1.2306346893310547\n",
      "Epoch: 128/300, Step: 1/4, Loss: 1.2179385423660278\n",
      "Epoch: 129/300, Step: 1/4, Loss: 1.2054529190063477\n",
      "Epoch: 130/300, Step: 1/4, Loss: 1.1931713819503784\n",
      "Epoch: 131/300, Step: 1/4, Loss: 1.1810901165008545\n",
      "Epoch: 132/300, Step: 1/4, Loss: 1.1692054271697998\n",
      "Epoch: 133/300, Step: 1/4, Loss: 1.1575102806091309\n",
      "Epoch: 134/300, Step: 1/4, Loss: 1.146001935005188\n",
      "Epoch: 135/300, Step: 1/4, Loss: 1.1346766948699951\n",
      "Epoch: 136/300, Step: 1/4, Loss: 1.1235301494598389\n",
      "Epoch: 137/300, Step: 1/4, Loss: 1.112558126449585\n",
      "Epoch: 138/300, Step: 1/4, Loss: 1.1017570495605469\n",
      "Epoch: 139/300, Step: 1/4, Loss: 1.0911226272583008\n",
      "Epoch: 140/300, Step: 1/4, Loss: 1.0806519985198975\n",
      "Epoch: 141/300, Step: 1/4, Loss: 1.0703418254852295\n",
      "Epoch: 142/300, Step: 1/4, Loss: 1.060187816619873\n",
      "Epoch: 143/300, Step: 1/4, Loss: 1.0501878261566162\n",
      "Epoch: 144/300, Step: 1/4, Loss: 1.0403378009796143\n",
      "Epoch: 145/300, Step: 1/4, Loss: 1.0306334495544434\n",
      "Epoch: 146/300, Step: 1/4, Loss: 1.021073579788208\n",
      "Epoch: 147/300, Step: 1/4, Loss: 1.0116544961929321\n",
      "Epoch: 148/300, Step: 1/4, Loss: 1.002374291419983\n",
      "Epoch: 149/300, Step: 1/4, Loss: 0.9932289123535156\n",
      "Epoch: 150/300, Step: 1/4, Loss: 0.9842157363891602\n",
      "Epoch: 151/300, Step: 1/4, Loss: 0.9753327369689941\n",
      "Epoch: 152/300, Step: 1/4, Loss: 0.9665769934654236\n",
      "Epoch: 153/300, Step: 1/4, Loss: 0.9579458236694336\n",
      "Epoch: 154/300, Step: 1/4, Loss: 0.9494362473487854\n",
      "Epoch: 155/300, Step: 1/4, Loss: 0.9410467743873596\n",
      "Epoch: 156/300, Step: 1/4, Loss: 0.9327752590179443\n",
      "Epoch: 157/300, Step: 1/4, Loss: 0.9246186017990112\n",
      "Epoch: 158/300, Step: 1/4, Loss: 0.91657555103302\n",
      "Epoch: 159/300, Step: 1/4, Loss: 0.9086418747901917\n",
      "Epoch: 160/300, Step: 1/4, Loss: 0.9008187651634216\n",
      "Epoch: 161/300, Step: 1/4, Loss: 0.893100380897522\n",
      "Epoch: 162/300, Step: 1/4, Loss: 0.8854883313179016\n",
      "Epoch: 163/300, Step: 1/4, Loss: 0.8779772520065308\n",
      "Epoch: 164/300, Step: 1/4, Loss: 0.870567798614502\n",
      "Epoch: 165/300, Step: 1/4, Loss: 0.863257110118866\n",
      "Epoch: 166/300, Step: 1/4, Loss: 0.8560436964035034\n",
      "Epoch: 167/300, Step: 1/4, Loss: 0.8489251136779785\n",
      "Epoch: 168/300, Step: 1/4, Loss: 0.8418999910354614\n",
      "Epoch: 169/300, Step: 1/4, Loss: 0.8349663019180298\n",
      "Epoch: 170/300, Step: 1/4, Loss: 0.8281230330467224\n",
      "Epoch: 171/300, Step: 1/4, Loss: 0.8213682770729065\n",
      "Epoch: 172/300, Step: 1/4, Loss: 0.8147009611129761\n",
      "Epoch: 173/300, Step: 1/4, Loss: 0.8081182837486267\n",
      "Epoch: 174/300, Step: 1/4, Loss: 0.8016201853752136\n",
      "Epoch: 175/300, Step: 1/4, Loss: 0.7952035665512085\n",
      "Epoch: 176/300, Step: 1/4, Loss: 0.7888683080673218\n",
      "Epoch: 177/300, Step: 1/4, Loss: 0.7826131582260132\n",
      "Epoch: 178/300, Step: 1/4, Loss: 0.7764345407485962\n",
      "Epoch: 179/300, Step: 1/4, Loss: 0.7703335881233215\n",
      "Epoch: 180/300, Step: 1/4, Loss: 0.7643095254898071\n",
      "Epoch: 181/300, Step: 1/4, Loss: 0.7583576440811157\n",
      "Epoch: 182/300, Step: 1/4, Loss: 0.7524796724319458\n",
      "Epoch: 183/300, Step: 1/4, Loss: 0.7466724514961243\n",
      "Epoch: 184/300, Step: 1/4, Loss: 0.7409369945526123\n",
      "Epoch: 185/300, Step: 1/4, Loss: 0.735270082950592\n",
      "Epoch: 186/300, Step: 1/4, Loss: 0.7296717762947083\n",
      "Epoch: 187/300, Step: 1/4, Loss: 0.7241389751434326\n",
      "Epoch: 188/300, Step: 1/4, Loss: 0.7186731100082397\n",
      "Epoch: 189/300, Step: 1/4, Loss: 0.7132729291915894\n",
      "Epoch: 190/300, Step: 1/4, Loss: 0.7079355120658875\n",
      "Epoch: 191/300, Step: 1/4, Loss: 0.7026605606079102\n",
      "Epoch: 192/300, Step: 1/4, Loss: 0.6974480748176575\n",
      "Epoch: 193/300, Step: 1/4, Loss: 0.692295491695404\n",
      "Epoch: 194/300, Step: 1/4, Loss: 0.6872036457061768\n",
      "Epoch: 195/300, Step: 1/4, Loss: 0.6821696758270264\n",
      "Epoch: 196/300, Step: 1/4, Loss: 0.6771937608718872\n",
      "Epoch: 197/300, Step: 1/4, Loss: 0.6722751259803772\n",
      "Epoch: 198/300, Step: 1/4, Loss: 0.6674124002456665\n",
      "Epoch: 199/300, Step: 1/4, Loss: 0.662604570388794\n",
      "Epoch: 200/300, Step: 1/4, Loss: 0.6578514575958252\n",
      "Epoch: 201/300, Step: 1/4, Loss: 0.6531503796577454\n",
      "Epoch: 202/300, Step: 1/4, Loss: 0.6485038995742798\n",
      "Epoch: 203/300, Step: 1/4, Loss: 0.6439076662063599\n",
      "Epoch: 204/300, Step: 1/4, Loss: 0.6393636465072632\n",
      "Epoch: 205/300, Step: 1/4, Loss: 0.6348692178726196\n",
      "Epoch: 206/300, Step: 1/4, Loss: 0.6304237842559814\n",
      "Epoch: 207/300, Step: 1/4, Loss: 0.6260281801223755\n",
      "Epoch: 208/300, Step: 1/4, Loss: 0.621679425239563\n",
      "Epoch: 209/300, Step: 1/4, Loss: 0.6173788905143738\n",
      "Epoch: 210/300, Step: 1/4, Loss: 0.6131244897842407\n",
      "Epoch: 211/300, Step: 1/4, Loss: 0.6089155673980713\n",
      "Epoch: 212/300, Step: 1/4, Loss: 0.6047526597976685\n",
      "Epoch: 213/300, Step: 1/4, Loss: 0.6006332635879517\n",
      "Epoch: 214/300, Step: 1/4, Loss: 0.5965587496757507\n",
      "Epoch: 215/300, Step: 1/4, Loss: 0.5925270318984985\n",
      "Epoch: 216/300, Step: 1/4, Loss: 0.5885382294654846\n",
      "Epoch: 217/300, Step: 1/4, Loss: 0.5845914483070374\n",
      "Epoch: 218/300, Step: 1/4, Loss: 0.5806849002838135\n",
      "Epoch: 219/300, Step: 1/4, Loss: 0.5768197774887085\n",
      "Epoch: 220/300, Step: 1/4, Loss: 0.5729950666427612\n",
      "Epoch: 221/300, Step: 1/4, Loss: 0.5692099928855896\n",
      "Epoch: 222/300, Step: 1/4, Loss: 0.5654643774032593\n",
      "Epoch: 223/300, Step: 1/4, Loss: 0.5617565512657166\n",
      "Epoch: 224/300, Step: 1/4, Loss: 0.5580872297286987\n",
      "Epoch: 225/300, Step: 1/4, Loss: 0.554454505443573\n",
      "Epoch: 226/300, Step: 1/4, Loss: 0.5508596897125244\n",
      "Epoch: 227/300, Step: 1/4, Loss: 0.5473001003265381\n",
      "Epoch: 228/300, Step: 1/4, Loss: 0.5437777638435364\n",
      "Epoch: 229/300, Step: 1/4, Loss: 0.5402894020080566\n",
      "Epoch: 230/300, Step: 1/4, Loss: 0.5368374586105347\n",
      "Epoch: 231/300, Step: 1/4, Loss: 0.5334186553955078\n",
      "Epoch: 232/300, Step: 1/4, Loss: 0.5300345420837402\n",
      "Epoch: 233/300, Step: 1/4, Loss: 0.5266835689544678\n",
      "Epoch: 234/300, Step: 1/4, Loss: 0.5233659744262695\n",
      "Epoch: 235/300, Step: 1/4, Loss: 0.5200799107551575\n",
      "Epoch: 236/300, Step: 1/4, Loss: 0.5168270468711853\n",
      "Epoch: 237/300, Step: 1/4, Loss: 0.513606071472168\n",
      "Epoch: 238/300, Step: 1/4, Loss: 0.5104164481163025\n",
      "Epoch: 239/300, Step: 1/4, Loss: 0.5072572231292725\n",
      "Epoch: 240/300, Step: 1/4, Loss: 0.5041296482086182\n",
      "Epoch: 241/300, Step: 1/4, Loss: 0.5010302066802979\n",
      "Epoch: 242/300, Step: 1/4, Loss: 0.4979608356952667\n",
      "Epoch: 243/300, Step: 1/4, Loss: 0.4949227273464203\n",
      "Epoch: 244/300, Step: 1/4, Loss: 0.49191024899482727\n",
      "Epoch: 245/300, Step: 1/4, Loss: 0.48892876505851746\n",
      "Epoch: 246/300, Step: 1/4, Loss: 0.48597532510757446\n",
      "Epoch: 247/300, Step: 1/4, Loss: 0.48304885625839233\n",
      "Epoch: 248/300, Step: 1/4, Loss: 0.4801501929759979\n",
      "Epoch: 249/300, Step: 1/4, Loss: 0.4772791564464569\n",
      "Epoch: 250/300, Step: 1/4, Loss: 0.4744335114955902\n",
      "Epoch: 251/300, Step: 1/4, Loss: 0.4716159701347351\n",
      "Epoch: 252/300, Step: 1/4, Loss: 0.4688229560852051\n",
      "Epoch: 253/300, Step: 1/4, Loss: 0.4660567343235016\n",
      "Epoch: 254/300, Step: 1/4, Loss: 0.46331533789634705\n",
      "Epoch: 255/300, Step: 1/4, Loss: 0.4605996310710907\n",
      "Epoch: 256/300, Step: 1/4, Loss: 0.45790714025497437\n",
      "Epoch: 257/300, Step: 1/4, Loss: 0.45524123311042786\n",
      "Epoch: 258/300, Step: 1/4, Loss: 0.4525986611843109\n",
      "Epoch: 259/300, Step: 1/4, Loss: 0.449980229139328\n",
      "Epoch: 260/300, Step: 1/4, Loss: 0.44738510251045227\n",
      "Epoch: 261/300, Step: 1/4, Loss: 0.44481220841407776\n",
      "Epoch: 262/300, Step: 1/4, Loss: 0.4422639310359955\n",
      "Epoch: 263/300, Step: 1/4, Loss: 0.4397379159927368\n",
      "Epoch: 264/300, Step: 1/4, Loss: 0.43723422288894653\n",
      "Epoch: 265/300, Step: 1/4, Loss: 0.4347520172595978\n",
      "Epoch: 266/300, Step: 1/4, Loss: 0.43229249119758606\n",
      "Epoch: 267/300, Step: 1/4, Loss: 0.42985421419143677\n",
      "Epoch: 268/300, Step: 1/4, Loss: 0.42743849754333496\n",
      "Epoch: 269/300, Step: 1/4, Loss: 0.4250413775444031\n",
      "Epoch: 270/300, Step: 1/4, Loss: 0.4226674437522888\n",
      "Epoch: 271/300, Step: 1/4, Loss: 0.42031288146972656\n",
      "Epoch: 272/300, Step: 1/4, Loss: 0.4179796278476715\n",
      "Epoch: 273/300, Step: 1/4, Loss: 0.4156656265258789\n",
      "Epoch: 274/300, Step: 1/4, Loss: 0.41337263584136963\n",
      "Epoch: 275/300, Step: 1/4, Loss: 0.4110981822013855\n",
      "Epoch: 276/300, Step: 1/4, Loss: 0.4088435769081116\n",
      "Epoch: 277/300, Step: 1/4, Loss: 0.4066082239151001\n",
      "Epoch: 278/300, Step: 1/4, Loss: 0.4043916165828705\n",
      "Epoch: 279/300, Step: 1/4, Loss: 0.4021948575973511\n",
      "Epoch: 280/300, Step: 1/4, Loss: 0.4000154733657837\n",
      "Epoch: 281/300, Step: 1/4, Loss: 0.39785391092300415\n",
      "Epoch: 282/300, Step: 1/4, Loss: 0.395712673664093\n",
      "Epoch: 283/300, Step: 1/4, Loss: 0.39358675479888916\n",
      "Epoch: 284/300, Step: 1/4, Loss: 0.3914807140827179\n",
      "Epoch: 285/300, Step: 1/4, Loss: 0.3893906772136688\n",
      "Epoch: 286/300, Step: 1/4, Loss: 0.3873179852962494\n",
      "Epoch: 287/300, Step: 1/4, Loss: 0.38526320457458496\n",
      "Epoch: 288/300, Step: 1/4, Loss: 0.38322553038597107\n",
      "Epoch: 289/300, Step: 1/4, Loss: 0.38120409846305847\n",
      "Epoch: 290/300, Step: 1/4, Loss: 0.3791992664337158\n",
      "Epoch: 291/300, Step: 1/4, Loss: 0.3772115707397461\n",
      "Epoch: 292/300, Step: 1/4, Loss: 0.3752387762069702\n",
      "Epoch: 293/300, Step: 1/4, Loss: 0.3732817769050598\n",
      "Epoch: 294/300, Step: 1/4, Loss: 0.3713415265083313\n",
      "Epoch: 295/300, Step: 1/4, Loss: 0.3694162666797638\n",
      "Epoch: 296/300, Step: 1/4, Loss: 0.3675069212913513\n",
      "Epoch: 297/300, Step: 1/4, Loss: 0.3656126856803894\n",
      "Epoch: 298/300, Step: 1/4, Loss: 0.3637342154979706\n",
      "Epoch: 299/300, Step: 1/4, Loss: 0.361869752407074\n",
      "Epoch: 300/300, Step: 1/4, Loss: 0.3600198030471802\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 300\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        model_inputs = {\n",
    "        \"input_ids\": batch[\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"attention_mask\"].to(device),\n",
    "        \"pixel_values\": batch[\"pixel_values\"].to(device),\n",
    "        }\n",
    "\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        model_inputs['labels'] = labels\n",
    "\n",
    "        outputs = model(**model_inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{len(batch['input_ids'])}, Loss: {outputs.loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:24:45.646371Z",
     "iopub.status.busy": "2024-05-10T18:24:45.645416Z",
     "iopub.status.idle": "2024-05-10T18:24:46.281441Z",
     "shell.execute_reply": "2024-05-10T18:24:46.280413Z",
     "shell.execute_reply.started": "2024-05-10T18:24:45.646335Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "path = \"my_fine_tuned_model.pth\"\n",
    "save_model(model, path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T18:55:06.405790Z",
     "iopub.status.busy": "2024-05-10T18:55:06.405114Z",
     "iopub.status.idle": "2024-05-10T18:55:06.410035Z",
     "shell.execute_reply": "2024-05-10T18:55:06.409113Z",
     "shell.execute_reply.started": "2024-05-10T18:55:06.405759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import ViltProcessor, ViltForQuestionAnswering\n",
    "\n",
    "def load_model(model_class, path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model_class()\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode for inference\n",
    "    return model\n",
    "\n",
    "path = '/kaggle/working/my_fine_tuned_model.pth'\n",
    "model = load_model(ViltForQuestionAnswering(config.id2label = id2label,config.label2id = label2id), path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-05-10T19:08:05.876144Z",
     "iopub.status.busy": "2024-05-10T19:08:05.875442Z",
     "iopub.status.idle": "2024-05-10T19:08:06.215363Z",
     "shell.execute_reply": "2024-05-10T19:08:06.214289Z",
     "shell.execute_reply.started": "2024-05-10T19:08:05.876112Z"
    },
    "id": "2ECQ6af5ipe2",
    "outputId": "f5471747-503b-46ad-ae6f-b8127ce8ac19",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "indexes = [19,545,712,668,109,545,888]\n",
    "all_logits = []\n",
    "all_outs = []\n",
    "all_answers = []\n",
    "for index in indexes:\n",
    "    image = dataset['image'][index]\n",
    "    question = dataset['question'][index]\n",
    "    answer = dataset['answer'][index]\n",
    "    weight = dataset['weight'][index]\n",
    "    all_answers.append(replace_ids(answer))\n",
    "    example = processor(image,question,return_tensors = 'pt')\n",
    "    example.to(device)\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(**example)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    all_logits.append(logits)\n",
    "    predicted_classes = torch.softmax(logits,dim = 1)\n",
    "\n",
    "    probs, classes = torch.topk(predicted_classes, 5)\n",
    "    maximum = 0\n",
    "    out = \"\"\n",
    "    for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n",
    "        if prob > maximum:\n",
    "            out = model.config.id2label[class_idx]\n",
    "            maximum = prob\n",
    "            \n",
    "    all_outs.append(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-10T19:08:08.353914Z",
     "iopub.status.busy": "2024-05-10T19:08:08.353272Z",
     "iopub.status.idle": "2024-05-10T19:08:08.359784Z",
     "shell.execute_reply": "2024-05-10T19:08:08.358797Z",
     "shell.execute_reply.started": "2024-05-10T19:08:08.353885Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pa',\n",
       " '5.6cm focal, predominantly hypodense',\n",
       " 'hydropneumothorax',\n",
       " 'yes',\n",
       " 'axial',\n",
       " '5.6cm focal, predominantly hypodense',\n",
       " 'yes']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
