{"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers\n!pip install datasets","metadata":{"id":"LXCtXJ1WTtJH","execution":{"iopub.status.busy":"2024-05-10T18:12:43.222198Z","iopub.execute_input":"2024-05-10T18:12:43.222537Z","iopub.status.idle":"2024-05-10T18:13:10.565965Z","shell.execute_reply.started":"2024-05-10T18:12:43.222510Z","shell.execute_reply":"2024-05-10T18:13:10.565055Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.18.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets) (3.13.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (15.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub>=0.19.4 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.22.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (6.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import ViltProcessor, ViltForQuestionAnswering\nimport requests\nfrom PIL import Image\nfrom datasets import load_dataset\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport itertools","metadata":{"id":"7LIqrrVUTuJC","execution":{"iopub.status.busy":"2024-05-10T18:13:45.734962Z","iopub.execute_input":"2024-05-10T18:13:45.735352Z","iopub.status.idle":"2024-05-10T18:13:58.494033Z","shell.execute_reply.started":"2024-05-10T18:13:45.735319Z","shell.execute_reply":"2024-05-10T18:13:58.493094Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Set device and move model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uXssIJhKmiN8","outputId":"7e0a8fa7-ba8c-4c24-85e5-a41c5cf6bd64","execution":{"iopub.status.busy":"2024-05-10T18:13:58.495561Z","iopub.execute_input":"2024-05-10T18:13:58.496009Z","iopub.status.idle":"2024-05-10T18:13:58.558010Z","shell.execute_reply.started":"2024-05-10T18:13:58.495983Z","shell.execute_reply":"2024-05-10T18:13:58.556937Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"class VQADataset(Dataset):\n    \n    def __init__(self, data, processor,max_length):\n        # Load your data from the provided path (replace with your logic)\n        self.data = data\n        self.processor = processor\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        \n        image = self.data['image'][idx]\n        questions = self.data['question'][idx]\n\n        encoding = self.processor(image, questions, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n        # remove batch dimension\n        for k,v in encoding.items():\n            encoding[k] = v.squeeze()\n        # add labels\n        labels = self.data['answer'][idx]\n        scores = self.data['weight'][idx]\n        targets = torch.zeros(len(id2label))\n        targets[labels] = scores\n        encoding[\"labels\"] = targets\n\n        return encoding","metadata":{"id":"aLv5G307T0DD","execution":{"iopub.status.busy":"2024-05-10T18:16:04.716525Z","iopub.execute_input":"2024-05-10T18:16:04.717134Z","iopub.status.idle":"2024-05-10T18:16:04.726247Z","shell.execute_reply.started":"2024-05-10T18:16:04.717101Z","shell.execute_reply":"2024-05-10T18:16:04.725267Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"processor = ViltProcessor.from_pretrained(\"dandelin/vilt-b32-mlm\")","metadata":{"id":"dt8ymApdT2Q8","execution":{"iopub.status.busy":"2024-05-10T18:17:20.235201Z","iopub.execute_input":"2024-05-10T18:17:20.235577Z","iopub.status.idle":"2024-05-10T18:17:36.613417Z","shell.execute_reply.started":"2024-05-10T18:17:20.235549Z","shell.execute_reply":"2024-05-10T18:17:36.612659Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-05-10 18:17:24.219525: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-10 18:17:24.219650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-10 18:17:24.471035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/251 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09c7db114d5e4f68bb5f6a4169eed0d2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/320 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6146d8d3ab114983add1fc81623c2bcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"423b51d7e996412a8f52a1f0476c9b06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"769cb8006880458493bd809430bd5186"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba84bb2182a48f380d0379b8afa033e"}},"metadata":{}}]},{"cell_type":"code","source":"dataset = load_dataset(\"flaviagiammarino/vqa-rad\")['train'][:]\ndataset['weight'] = torch.ones(len(dataset['image']))\n\n\ndef replace_str(inputs):\n    return label2id[inputs]\n\ndef replace_ids(inputs):\n    return id2label[inputs]\n\nlabels = dataset['answer']\n\nunique_labels = []\n\nfor label in labels:\n    if label not in unique_labels:\n        unique_labels.append(label)\n\n\n\nlabel2id = {\"<S>\":0,\"<E>\":1}\nnum_id = 2\nfor label in unique_labels:\n    label2id[label] = num_id\n    num_id+=1\n\nlabel2id\n\nid2label = {idx: label for label, idx in label2id.items()}\n\n\nanswers = []\nfor data in dataset['answer']:\n    answers.append(replace_str(data))\n\ndataset['answer'] = answers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8BlIFjCrdbcA","outputId":"8aae2150-53d5-4573-f0cf-b5b28b080d7e","execution":{"iopub.status.busy":"2024-05-10T19:04:10.714988Z","iopub.execute_input":"2024-05-10T19:04:10.715760Z","iopub.status.idle":"2024-05-10T19:04:19.191898Z","shell.execute_reply.started":"2024-05-10T19:04:10.715720Z","shell.execute_reply":"2024-05-10T19:04:19.191109Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"id2label[101]","metadata":{"execution":{"iopub.status.busy":"2024-05-10T19:05:02.219621Z","iopub.execute_input":"2024-05-10T19:05:02.219965Z","iopub.status.idle":"2024-05-10T19:05:02.226030Z","shell.execute_reply.started":"2024-05-10T19:05:02.219940Z","shell.execute_reply":"2024-05-10T19:05:02.225132Z"},"trusted":true},"execution_count":90,"outputs":[{"execution_count":90,"output_type":"execute_result","data":{"text/plain":"'non-enhanced'"},"metadata":{}}]},{"cell_type":"code","source":"len(dataset['image'])","metadata":{"execution":{"iopub.status.busy":"2024-05-10T19:04:41.163009Z","iopub.execute_input":"2024-05-10T19:04:41.163933Z","iopub.status.idle":"2024-05-10T19:04:41.169505Z","shell.execute_reply.started":"2024-05-10T19:04:41.163895Z","shell.execute_reply":"2024-05-10T19:04:41.168489Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"1793"},"metadata":{}}]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel = ViltForQuestionAnswering.from_pretrained(\"dandelin/vilt-b32-mlm\",id2label=id2label, label2id=label2id)\n\nfor name, param in model.named_parameters():\n    if not name.startswith(\"classifier\"):\n        param.requires_grad = False\n\nmodel.classifier = torch.nn.Sequential(\n    nn.Linear(in_features=768, out_features=1536, bias=True),\n  nn.LayerNorm((1536,), eps=1e-05, elementwise_affine=True),\n  nn.GELU(approximate='none'),\n  nn.Linear(in_features=1536, out_features=len(id2label), bias=True),\n)\n\n\nmodel.to(device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tykxgjx8pDlb","outputId":"f2a1e061-dd5e-4448-f34d-ce34c50ce8d8","execution":{"iopub.status.busy":"2024-05-10T19:04:41.975195Z","iopub.execute_input":"2024-05-10T19:04:41.975625Z","iopub.status.idle":"2024-05-10T19:04:42.978060Z","shell.execute_reply.started":"2024-05-10T19:04:41.975597Z","shell.execute_reply":"2024-05-10T19:04:42.977072Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"Some weights of ViltForQuestionAnswering were not initialized from the model checkpoint at dandelin/vilt-b32-mlm and are newly initialized: ['classifier.0.bias', 'classifier.0.weight', 'classifier.1.bias', 'classifier.1.weight', 'classifier.3.bias', 'classifier.3.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":87,"output_type":"execute_result","data":{"text/plain":"ViltForQuestionAnswering(\n  (vilt): ViltModel(\n    (embeddings): ViltEmbeddings(\n      (text_embeddings): TextEmbeddings(\n        (word_embeddings): Embedding(30522, 768)\n        (position_embeddings): Embedding(40, 768)\n        (token_type_embeddings): Embedding(2, 768)\n        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (dropout): Dropout(p=0.0, inplace=False)\n      )\n      (patch_embeddings): ViltPatchEmbeddings(\n        (projection): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32))\n      )\n      (token_type_embeddings): Embedding(2, 768)\n      (dropout): Dropout(p=0.0, inplace=False)\n    )\n    (encoder): ViltEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ViltLayer(\n          (attention): ViltAttention(\n            (attention): ViltSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): ViltSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n          )\n          (intermediate): ViltIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ViltOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (dropout): Dropout(p=0.0, inplace=False)\n          )\n          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        )\n      )\n    )\n    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (pooler): ViltPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (classifier): Sequential(\n    (0): Linear(in_features=768, out_features=1536, bias=True)\n    (1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=1536, out_features=434, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"def collate_fn(batch):\n    input_ids = [item['input_ids'] for item in batch]\n    pixel_values = [item['pixel_values'] for item in batch]\n    attention_mask = [item['attention_mask'] for item in batch]\n    token_type_ids = [item['token_type_ids']for item in batch]\n    labels = [item['labels'] for item in batch]\n\n    my_dict = {'input_ids':input_ids,'attention_mask':attention_mask}\n\n    image_encoding = processor.image_processor.pad(pixel_values, return_tensors=\"pt\")\n    text_encoding = processor.tokenizer.pad(my_dict,padding = 'max_length',return_tensors = \"pt\")\n\n    batch = {}\n    batch['input_ids'] = text_encoding['input_ids']\n    batch['attention_mask'] = text_encoding['attention_mask']\n    batch['pixel_values'] = image_encoding['pixel_values']\n    batch['pixel_mask'] = image_encoding['pixel_mask']\n    batch['labels'] = torch.stack(labels)\n\n    return batch\n\n\ntrain_dataset = VQADataset(dataset, processor,max_length = max)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, collate_fn = collate_fn, batch_size=4,shuffle= True)","metadata":{"id":"WmrusPZ1Wm0G","execution":{"iopub.status.busy":"2024-05-10T19:04:42.979437Z","iopub.execute_input":"2024-05-10T19:04:42.979730Z","iopub.status.idle":"2024-05-10T19:04:42.988105Z","shell.execute_reply.started":"2024-05-10T19:04:42.979705Z","shell.execute_reply":"2024-05-10T19:04:42.987277Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"for i, batch in enumerate(train_loader):\n    print(batch['labels'])\n    ","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_iNl2hwNipnm","outputId":"cf548b9c-6969-498f-82ac-ea10e4b50a95","execution":{"iopub.status.busy":"2024-05-10T19:05:19.883827Z","iopub.execute_input":"2024-05-10T19:05:19.884544Z","iopub.status.idle":"2024-05-10T19:05:19.971095Z","shell.execute_reply.started":"2024-05-10T19:05:19.884514Z","shell.execute_reply":"2024-05-10T19:05:19.970073Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 1.,  ..., 0., 0., 0.]])\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 300\n\nmodel.train()\nfor epoch in range(num_epochs):\n    for i, batch in enumerate(train_loader):\n        optimizer.zero_grad()\n        model_inputs = {\n        \"input_ids\": batch[\"input_ids\"].to(device),\n        \"attention_mask\": batch[\"attention_mask\"].to(device),\n        \"pixel_values\": batch[\"pixel_values\"].to(device),\n        }\n\n        labels = batch[\"labels\"].to(device)\n\n        model_inputs['labels'] = labels\n\n        outputs = model(**model_inputs)\n        loss = outputs.loss\n        loss.backward()\n        optimizer.step()\n\n        print(f\"Epoch: {epoch+1}/{num_epochs}, Step: {i+1}/{len(batch['input_ids'])}, Loss: {outputs.loss.item()}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eraU_J2xipi4","outputId":"0968a616-3e9a-40c6-8260-9ce7316ce122","execution":{"iopub.status.busy":"2024-05-10T19:06:41.826559Z","iopub.execute_input":"2024-05-10T19:06:41.826923Z","iopub.status.idle":"2024-05-10T19:07:22.639337Z","shell.execute_reply.started":"2024-05-10T19:06:41.826893Z","shell.execute_reply":"2024-05-10T19:07:22.638380Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stdout","text":"Epoch: 1/300, Step: 1/4, Loss: 16.93903350830078\nEpoch: 2/300, Step: 1/4, Loss: 16.242759704589844\nEpoch: 3/300, Step: 1/4, Loss: 15.597319602966309\nEpoch: 4/300, Step: 1/4, Loss: 14.989378929138184\nEpoch: 5/300, Step: 1/4, Loss: 14.416669845581055\nEpoch: 6/300, Step: 1/4, Loss: 13.875\nEpoch: 7/300, Step: 1/4, Loss: 13.360313415527344\nEpoch: 8/300, Step: 1/4, Loss: 12.86999225616455\nEpoch: 9/300, Step: 1/4, Loss: 12.402443885803223\nEpoch: 10/300, Step: 1/4, Loss: 11.956439971923828\nEpoch: 11/300, Step: 1/4, Loss: 11.530654907226562\nEpoch: 12/300, Step: 1/4, Loss: 11.12398910522461\nEpoch: 13/300, Step: 1/4, Loss: 10.73564624786377\nEpoch: 14/300, Step: 1/4, Loss: 10.364906311035156\nEpoch: 15/300, Step: 1/4, Loss: 10.011009216308594\nEpoch: 16/300, Step: 1/4, Loss: 9.673189163208008\nEpoch: 17/300, Step: 1/4, Loss: 9.350709915161133\nEpoch: 18/300, Step: 1/4, Loss: 9.042869567871094\nEpoch: 19/300, Step: 1/4, Loss: 8.748971939086914\nEpoch: 20/300, Step: 1/4, Loss: 8.468330383300781\nEpoch: 21/300, Step: 1/4, Loss: 8.200295448303223\nEpoch: 22/300, Step: 1/4, Loss: 7.944252967834473\nEpoch: 23/300, Step: 1/4, Loss: 7.699598789215088\nEpoch: 24/300, Step: 1/4, Loss: 7.465758323669434\nEpoch: 25/300, Step: 1/4, Loss: 7.242187023162842\nEpoch: 26/300, Step: 1/4, Loss: 7.028370380401611\nEpoch: 27/300, Step: 1/4, Loss: 6.823818206787109\nEpoch: 28/300, Step: 1/4, Loss: 6.628042221069336\nEpoch: 29/300, Step: 1/4, Loss: 6.440584659576416\nEpoch: 30/300, Step: 1/4, Loss: 6.261001110076904\nEpoch: 31/300, Step: 1/4, Loss: 6.088878631591797\nEpoch: 32/300, Step: 1/4, Loss: 5.923818588256836\nEpoch: 33/300, Step: 1/4, Loss: 5.765441417694092\nEpoch: 34/300, Step: 1/4, Loss: 5.613379955291748\nEpoch: 35/300, Step: 1/4, Loss: 5.467298984527588\nEpoch: 36/300, Step: 1/4, Loss: 5.326879024505615\nEpoch: 37/300, Step: 1/4, Loss: 5.191822528839111\nEpoch: 38/300, Step: 1/4, Loss: 5.0618486404418945\nEpoch: 39/300, Step: 1/4, Loss: 4.9366936683654785\nEpoch: 40/300, Step: 1/4, Loss: 4.816119194030762\nEpoch: 41/300, Step: 1/4, Loss: 4.69989013671875\nEpoch: 42/300, Step: 1/4, Loss: 4.587800979614258\nEpoch: 43/300, Step: 1/4, Loss: 4.4796552658081055\nEpoch: 44/300, Step: 1/4, Loss: 4.375265121459961\nEpoch: 45/300, Step: 1/4, Loss: 4.274462699890137\nEpoch: 46/300, Step: 1/4, Loss: 4.177082061767578\nEpoch: 47/300, Step: 1/4, Loss: 4.082976818084717\nEpoch: 48/300, Step: 1/4, Loss: 3.9920082092285156\nEpoch: 49/300, Step: 1/4, Loss: 3.9040446281433105\nEpoch: 50/300, Step: 1/4, Loss: 3.818969964981079\nEpoch: 51/300, Step: 1/4, Loss: 3.736673355102539\nEpoch: 52/300, Step: 1/4, Loss: 3.657052755355835\nEpoch: 53/300, Step: 1/4, Loss: 3.580012083053589\nEpoch: 54/300, Step: 1/4, Loss: 3.505465030670166\nEpoch: 55/300, Step: 1/4, Loss: 3.433326005935669\nEpoch: 56/300, Step: 1/4, Loss: 3.3635177612304688\nEpoch: 57/300, Step: 1/4, Loss: 3.295964479446411\nEpoch: 58/300, Step: 1/4, Loss: 3.2305941581726074\nEpoch: 59/300, Step: 1/4, Loss: 3.167332649230957\nEpoch: 60/300, Step: 1/4, Loss: 3.1061131954193115\nEpoch: 61/300, Step: 1/4, Loss: 3.046865940093994\nEpoch: 62/300, Step: 1/4, Loss: 2.9895219802856445\nEpoch: 63/300, Step: 1/4, Loss: 2.9340128898620605\nEpoch: 64/300, Step: 1/4, Loss: 2.8802714347839355\nEpoch: 65/300, Step: 1/4, Loss: 2.8282313346862793\nEpoch: 66/300, Step: 1/4, Loss: 2.777827262878418\nEpoch: 67/300, Step: 1/4, Loss: 2.7289865016937256\nEpoch: 68/300, Step: 1/4, Loss: 2.681645393371582\nEpoch: 69/300, Step: 1/4, Loss: 2.635737419128418\nEpoch: 70/300, Step: 1/4, Loss: 2.591196060180664\nEpoch: 71/300, Step: 1/4, Loss: 2.547959327697754\nEpoch: 72/300, Step: 1/4, Loss: 2.5059657096862793\nEpoch: 73/300, Step: 1/4, Loss: 2.4651572704315186\nEpoch: 74/300, Step: 1/4, Loss: 2.425478458404541\nEpoch: 75/300, Step: 1/4, Loss: 2.386878490447998\nEpoch: 76/300, Step: 1/4, Loss: 2.3493099212646484\nEpoch: 77/300, Step: 1/4, Loss: 2.3127269744873047\nEpoch: 78/300, Step: 1/4, Loss: 2.2770886421203613\nEpoch: 79/300, Step: 1/4, Loss: 2.2423555850982666\nEpoch: 80/300, Step: 1/4, Loss: 2.2084951400756836\nEpoch: 81/300, Step: 1/4, Loss: 2.1754722595214844\nEpoch: 82/300, Step: 1/4, Loss: 2.143259286880493\nEpoch: 83/300, Step: 1/4, Loss: 2.1118276119232178\nEpoch: 84/300, Step: 1/4, Loss: 2.0811524391174316\nEpoch: 85/300, Step: 1/4, Loss: 2.051208257675171\nEpoch: 86/300, Step: 1/4, Loss: 2.021972417831421\nEpoch: 87/300, Step: 1/4, Loss: 1.9934204816818237\nEpoch: 88/300, Step: 1/4, Loss: 1.965531587600708\nEpoch: 89/300, Step: 1/4, Loss: 1.9382882118225098\nEpoch: 90/300, Step: 1/4, Loss: 1.9116662740707397\nEpoch: 91/300, Step: 1/4, Loss: 1.8856474161148071\nEpoch: 92/300, Step: 1/4, Loss: 1.8602123260498047\nEpoch: 93/300, Step: 1/4, Loss: 1.8353434801101685\nEpoch: 94/300, Step: 1/4, Loss: 1.8110222816467285\nEpoch: 95/300, Step: 1/4, Loss: 1.7872319221496582\nEpoch: 96/300, Step: 1/4, Loss: 1.7639563083648682\nEpoch: 97/300, Step: 1/4, Loss: 1.741176962852478\nEpoch: 98/300, Step: 1/4, Loss: 1.718881607055664\nEpoch: 99/300, Step: 1/4, Loss: 1.6970523595809937\nEpoch: 100/300, Step: 1/4, Loss: 1.6756762266159058\nEpoch: 101/300, Step: 1/4, Loss: 1.654740810394287\nEpoch: 102/300, Step: 1/4, Loss: 1.6342302560806274\nEpoch: 103/300, Step: 1/4, Loss: 1.6141341924667358\nEpoch: 104/300, Step: 1/4, Loss: 1.5944397449493408\nEpoch: 105/300, Step: 1/4, Loss: 1.575135588645935\nEpoch: 106/300, Step: 1/4, Loss: 1.5562105178833008\nEpoch: 107/300, Step: 1/4, Loss: 1.5376548767089844\nEpoch: 108/300, Step: 1/4, Loss: 1.5194581747055054\nEpoch: 109/300, Step: 1/4, Loss: 1.5016075372695923\nEpoch: 110/300, Step: 1/4, Loss: 1.4840996265411377\nEpoch: 111/300, Step: 1/4, Loss: 1.4669201374053955\nEpoch: 112/300, Step: 1/4, Loss: 1.4500619173049927\nEpoch: 113/300, Step: 1/4, Loss: 1.4335176944732666\nEpoch: 114/300, Step: 1/4, Loss: 1.4172773361206055\nEpoch: 115/300, Step: 1/4, Loss: 1.4013347625732422\nEpoch: 116/300, Step: 1/4, Loss: 1.3856815099716187\nEpoch: 117/300, Step: 1/4, Loss: 1.3703099489212036\nEpoch: 118/300, Step: 1/4, Loss: 1.355212926864624\nEpoch: 119/300, Step: 1/4, Loss: 1.3403847217559814\nEpoch: 120/300, Step: 1/4, Loss: 1.3258161544799805\nEpoch: 121/300, Step: 1/4, Loss: 1.3115034103393555\nEpoch: 122/300, Step: 1/4, Loss: 1.2974393367767334\nEpoch: 123/300, Step: 1/4, Loss: 1.283616304397583\nEpoch: 124/300, Step: 1/4, Loss: 1.2700304985046387\nEpoch: 125/300, Step: 1/4, Loss: 1.256676435470581\nEpoch: 126/300, Step: 1/4, Loss: 1.2435446977615356\nEpoch: 127/300, Step: 1/4, Loss: 1.2306346893310547\nEpoch: 128/300, Step: 1/4, Loss: 1.2179385423660278\nEpoch: 129/300, Step: 1/4, Loss: 1.2054529190063477\nEpoch: 130/300, Step: 1/4, Loss: 1.1931713819503784\nEpoch: 131/300, Step: 1/4, Loss: 1.1810901165008545\nEpoch: 132/300, Step: 1/4, Loss: 1.1692054271697998\nEpoch: 133/300, Step: 1/4, Loss: 1.1575102806091309\nEpoch: 134/300, Step: 1/4, Loss: 1.146001935005188\nEpoch: 135/300, Step: 1/4, Loss: 1.1346766948699951\nEpoch: 136/300, Step: 1/4, Loss: 1.1235301494598389\nEpoch: 137/300, Step: 1/4, Loss: 1.112558126449585\nEpoch: 138/300, Step: 1/4, Loss: 1.1017570495605469\nEpoch: 139/300, Step: 1/4, Loss: 1.0911226272583008\nEpoch: 140/300, Step: 1/4, Loss: 1.0806519985198975\nEpoch: 141/300, Step: 1/4, Loss: 1.0703418254852295\nEpoch: 142/300, Step: 1/4, Loss: 1.060187816619873\nEpoch: 143/300, Step: 1/4, Loss: 1.0501878261566162\nEpoch: 144/300, Step: 1/4, Loss: 1.0403378009796143\nEpoch: 145/300, Step: 1/4, Loss: 1.0306334495544434\nEpoch: 146/300, Step: 1/4, Loss: 1.021073579788208\nEpoch: 147/300, Step: 1/4, Loss: 1.0116544961929321\nEpoch: 148/300, Step: 1/4, Loss: 1.002374291419983\nEpoch: 149/300, Step: 1/4, Loss: 0.9932289123535156\nEpoch: 150/300, Step: 1/4, Loss: 0.9842157363891602\nEpoch: 151/300, Step: 1/4, Loss: 0.9753327369689941\nEpoch: 152/300, Step: 1/4, Loss: 0.9665769934654236\nEpoch: 153/300, Step: 1/4, Loss: 0.9579458236694336\nEpoch: 154/300, Step: 1/4, Loss: 0.9494362473487854\nEpoch: 155/300, Step: 1/4, Loss: 0.9410467743873596\nEpoch: 156/300, Step: 1/4, Loss: 0.9327752590179443\nEpoch: 157/300, Step: 1/4, Loss: 0.9246186017990112\nEpoch: 158/300, Step: 1/4, Loss: 0.91657555103302\nEpoch: 159/300, Step: 1/4, Loss: 0.9086418747901917\nEpoch: 160/300, Step: 1/4, Loss: 0.9008187651634216\nEpoch: 161/300, Step: 1/4, Loss: 0.893100380897522\nEpoch: 162/300, Step: 1/4, Loss: 0.8854883313179016\nEpoch: 163/300, Step: 1/4, Loss: 0.8779772520065308\nEpoch: 164/300, Step: 1/4, Loss: 0.870567798614502\nEpoch: 165/300, Step: 1/4, Loss: 0.863257110118866\nEpoch: 166/300, Step: 1/4, Loss: 0.8560436964035034\nEpoch: 167/300, Step: 1/4, Loss: 0.8489251136779785\nEpoch: 168/300, Step: 1/4, Loss: 0.8418999910354614\nEpoch: 169/300, Step: 1/4, Loss: 0.8349663019180298\nEpoch: 170/300, Step: 1/4, Loss: 0.8281230330467224\nEpoch: 171/300, Step: 1/4, Loss: 0.8213682770729065\nEpoch: 172/300, Step: 1/4, Loss: 0.8147009611129761\nEpoch: 173/300, Step: 1/4, Loss: 0.8081182837486267\nEpoch: 174/300, Step: 1/4, Loss: 0.8016201853752136\nEpoch: 175/300, Step: 1/4, Loss: 0.7952035665512085\nEpoch: 176/300, Step: 1/4, Loss: 0.7888683080673218\nEpoch: 177/300, Step: 1/4, Loss: 0.7826131582260132\nEpoch: 178/300, Step: 1/4, Loss: 0.7764345407485962\nEpoch: 179/300, Step: 1/4, Loss: 0.7703335881233215\nEpoch: 180/300, Step: 1/4, Loss: 0.7643095254898071\nEpoch: 181/300, Step: 1/4, Loss: 0.7583576440811157\nEpoch: 182/300, Step: 1/4, Loss: 0.7524796724319458\nEpoch: 183/300, Step: 1/4, Loss: 0.7466724514961243\nEpoch: 184/300, Step: 1/4, Loss: 0.7409369945526123\nEpoch: 185/300, Step: 1/4, Loss: 0.735270082950592\nEpoch: 186/300, Step: 1/4, Loss: 0.7296717762947083\nEpoch: 187/300, Step: 1/4, Loss: 0.7241389751434326\nEpoch: 188/300, Step: 1/4, Loss: 0.7186731100082397\nEpoch: 189/300, Step: 1/4, Loss: 0.7132729291915894\nEpoch: 190/300, Step: 1/4, Loss: 0.7079355120658875\nEpoch: 191/300, Step: 1/4, Loss: 0.7026605606079102\nEpoch: 192/300, Step: 1/4, Loss: 0.6974480748176575\nEpoch: 193/300, Step: 1/4, Loss: 0.692295491695404\nEpoch: 194/300, Step: 1/4, Loss: 0.6872036457061768\nEpoch: 195/300, Step: 1/4, Loss: 0.6821696758270264\nEpoch: 196/300, Step: 1/4, Loss: 0.6771937608718872\nEpoch: 197/300, Step: 1/4, Loss: 0.6722751259803772\nEpoch: 198/300, Step: 1/4, Loss: 0.6674124002456665\nEpoch: 199/300, Step: 1/4, Loss: 0.662604570388794\nEpoch: 200/300, Step: 1/4, Loss: 0.6578514575958252\nEpoch: 201/300, Step: 1/4, Loss: 0.6531503796577454\nEpoch: 202/300, Step: 1/4, Loss: 0.6485038995742798\nEpoch: 203/300, Step: 1/4, Loss: 0.6439076662063599\nEpoch: 204/300, Step: 1/4, Loss: 0.6393636465072632\nEpoch: 205/300, Step: 1/4, Loss: 0.6348692178726196\nEpoch: 206/300, Step: 1/4, Loss: 0.6304237842559814\nEpoch: 207/300, Step: 1/4, Loss: 0.6260281801223755\nEpoch: 208/300, Step: 1/4, Loss: 0.621679425239563\nEpoch: 209/300, Step: 1/4, Loss: 0.6173788905143738\nEpoch: 210/300, Step: 1/4, Loss: 0.6131244897842407\nEpoch: 211/300, Step: 1/4, Loss: 0.6089155673980713\nEpoch: 212/300, Step: 1/4, Loss: 0.6047526597976685\nEpoch: 213/300, Step: 1/4, Loss: 0.6006332635879517\nEpoch: 214/300, Step: 1/4, Loss: 0.5965587496757507\nEpoch: 215/300, Step: 1/4, Loss: 0.5925270318984985\nEpoch: 216/300, Step: 1/4, Loss: 0.5885382294654846\nEpoch: 217/300, Step: 1/4, Loss: 0.5845914483070374\nEpoch: 218/300, Step: 1/4, Loss: 0.5806849002838135\nEpoch: 219/300, Step: 1/4, Loss: 0.5768197774887085\nEpoch: 220/300, Step: 1/4, Loss: 0.5729950666427612\nEpoch: 221/300, Step: 1/4, Loss: 0.5692099928855896\nEpoch: 222/300, Step: 1/4, Loss: 0.5654643774032593\nEpoch: 223/300, Step: 1/4, Loss: 0.5617565512657166\nEpoch: 224/300, Step: 1/4, Loss: 0.5580872297286987\nEpoch: 225/300, Step: 1/4, Loss: 0.554454505443573\nEpoch: 226/300, Step: 1/4, Loss: 0.5508596897125244\nEpoch: 227/300, Step: 1/4, Loss: 0.5473001003265381\nEpoch: 228/300, Step: 1/4, Loss: 0.5437777638435364\nEpoch: 229/300, Step: 1/4, Loss: 0.5402894020080566\nEpoch: 230/300, Step: 1/4, Loss: 0.5368374586105347\nEpoch: 231/300, Step: 1/4, Loss: 0.5334186553955078\nEpoch: 232/300, Step: 1/4, Loss: 0.5300345420837402\nEpoch: 233/300, Step: 1/4, Loss: 0.5266835689544678\nEpoch: 234/300, Step: 1/4, Loss: 0.5233659744262695\nEpoch: 235/300, Step: 1/4, Loss: 0.5200799107551575\nEpoch: 236/300, Step: 1/4, Loss: 0.5168270468711853\nEpoch: 237/300, Step: 1/4, Loss: 0.513606071472168\nEpoch: 238/300, Step: 1/4, Loss: 0.5104164481163025\nEpoch: 239/300, Step: 1/4, Loss: 0.5072572231292725\nEpoch: 240/300, Step: 1/4, Loss: 0.5041296482086182\nEpoch: 241/300, Step: 1/4, Loss: 0.5010302066802979\nEpoch: 242/300, Step: 1/4, Loss: 0.4979608356952667\nEpoch: 243/300, Step: 1/4, Loss: 0.4949227273464203\nEpoch: 244/300, Step: 1/4, Loss: 0.49191024899482727\nEpoch: 245/300, Step: 1/4, Loss: 0.48892876505851746\nEpoch: 246/300, Step: 1/4, Loss: 0.48597532510757446\nEpoch: 247/300, Step: 1/4, Loss: 0.48304885625839233\nEpoch: 248/300, Step: 1/4, Loss: 0.4801501929759979\nEpoch: 249/300, Step: 1/4, Loss: 0.4772791564464569\nEpoch: 250/300, Step: 1/4, Loss: 0.4744335114955902\nEpoch: 251/300, Step: 1/4, Loss: 0.4716159701347351\nEpoch: 252/300, Step: 1/4, Loss: 0.4688229560852051\nEpoch: 253/300, Step: 1/4, Loss: 0.4660567343235016\nEpoch: 254/300, Step: 1/4, Loss: 0.46331533789634705\nEpoch: 255/300, Step: 1/4, Loss: 0.4605996310710907\nEpoch: 256/300, Step: 1/4, Loss: 0.45790714025497437\nEpoch: 257/300, Step: 1/4, Loss: 0.45524123311042786\nEpoch: 258/300, Step: 1/4, Loss: 0.4525986611843109\nEpoch: 259/300, Step: 1/4, Loss: 0.449980229139328\nEpoch: 260/300, Step: 1/4, Loss: 0.44738510251045227\nEpoch: 261/300, Step: 1/4, Loss: 0.44481220841407776\nEpoch: 262/300, Step: 1/4, Loss: 0.4422639310359955\nEpoch: 263/300, Step: 1/4, Loss: 0.4397379159927368\nEpoch: 264/300, Step: 1/4, Loss: 0.43723422288894653\nEpoch: 265/300, Step: 1/4, Loss: 0.4347520172595978\nEpoch: 266/300, Step: 1/4, Loss: 0.43229249119758606\nEpoch: 267/300, Step: 1/4, Loss: 0.42985421419143677\nEpoch: 268/300, Step: 1/4, Loss: 0.42743849754333496\nEpoch: 269/300, Step: 1/4, Loss: 0.4250413775444031\nEpoch: 270/300, Step: 1/4, Loss: 0.4226674437522888\nEpoch: 271/300, Step: 1/4, Loss: 0.42031288146972656\nEpoch: 272/300, Step: 1/4, Loss: 0.4179796278476715\nEpoch: 273/300, Step: 1/4, Loss: 0.4156656265258789\nEpoch: 274/300, Step: 1/4, Loss: 0.41337263584136963\nEpoch: 275/300, Step: 1/4, Loss: 0.4110981822013855\nEpoch: 276/300, Step: 1/4, Loss: 0.4088435769081116\nEpoch: 277/300, Step: 1/4, Loss: 0.4066082239151001\nEpoch: 278/300, Step: 1/4, Loss: 0.4043916165828705\nEpoch: 279/300, Step: 1/4, Loss: 0.4021948575973511\nEpoch: 280/300, Step: 1/4, Loss: 0.4000154733657837\nEpoch: 281/300, Step: 1/4, Loss: 0.39785391092300415\nEpoch: 282/300, Step: 1/4, Loss: 0.395712673664093\nEpoch: 283/300, Step: 1/4, Loss: 0.39358675479888916\nEpoch: 284/300, Step: 1/4, Loss: 0.3914807140827179\nEpoch: 285/300, Step: 1/4, Loss: 0.3893906772136688\nEpoch: 286/300, Step: 1/4, Loss: 0.3873179852962494\nEpoch: 287/300, Step: 1/4, Loss: 0.38526320457458496\nEpoch: 288/300, Step: 1/4, Loss: 0.38322553038597107\nEpoch: 289/300, Step: 1/4, Loss: 0.38120409846305847\nEpoch: 290/300, Step: 1/4, Loss: 0.3791992664337158\nEpoch: 291/300, Step: 1/4, Loss: 0.3772115707397461\nEpoch: 292/300, Step: 1/4, Loss: 0.3752387762069702\nEpoch: 293/300, Step: 1/4, Loss: 0.3732817769050598\nEpoch: 294/300, Step: 1/4, Loss: 0.3713415265083313\nEpoch: 295/300, Step: 1/4, Loss: 0.3694162666797638\nEpoch: 296/300, Step: 1/4, Loss: 0.3675069212913513\nEpoch: 297/300, Step: 1/4, Loss: 0.3656126856803894\nEpoch: 298/300, Step: 1/4, Loss: 0.3637342154979706\nEpoch: 299/300, Step: 1/4, Loss: 0.361869752407074\nEpoch: 300/300, Step: 1/4, Loss: 0.3600198030471802\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_model(model, path):\n    torch.save(model.state_dict(), path)\n\npath = \"my_fine_tuned_model.pth\"\nsave_model(model, path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T18:24:45.645416Z","iopub.execute_input":"2024-05-10T18:24:45.646371Z","iopub.status.idle":"2024-05-10T18:24:46.281441Z","shell.execute_reply.started":"2024-05-10T18:24:45.646335Z","shell.execute_reply":"2024-05-10T18:24:46.280413Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import ViltProcessor, ViltForQuestionAnswering\n\ndef load_model(model_class, path):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model_class()\n    model.load_state_dict(torch.load(path, map_location=device))\n    model.eval()  # Set the model to evaluation mode for inference\n    return model\n\npath = '/kaggle/working/my_fine_tuned_model.pth'\nmodel = load_model(ViltForQuestionAnswering(config.id2label = id2label,config.label2id = label2id), path)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-10T18:55:06.405114Z","iopub.execute_input":"2024-05-10T18:55:06.405790Z","iopub.status.idle":"2024-05-10T18:55:06.410035Z","shell.execute_reply.started":"2024-05-10T18:55:06.405759Z","shell.execute_reply":"2024-05-10T18:55:06.409113Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"\n\nindexes = [19,545,712,668,109,545,888]\nall_logits = []\nall_outs = []\nall_answers = []\nfor index in indexes:\n    image = dataset['image'][index]\n    question = dataset['question'][index]\n    answer = dataset['answer'][index]\n    weight = dataset['weight'][index]\n    all_answers.append(replace_ids(answer))\n    example = processor(image,question,return_tensors = 'pt')\n    example.to(device)\n    \n    # forward pass\n    outputs = model(**example)\n\n    logits = outputs.logits\n    all_logits.append(logits)\n    predicted_classes = torch.softmax(logits,dim = 1)\n\n    probs, classes = torch.topk(predicted_classes, 5)\n    maximum = 0\n    out = \"\"\n    for prob, class_idx in zip(probs.squeeze().tolist(), classes.squeeze().tolist()):\n        if prob > maximum:\n            out = model.config.id2label[class_idx]\n            maximum = prob\n            \n    all_outs.append(out)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ECQ6af5ipe2","outputId":"f5471747-503b-46ad-ae6f-b8127ce8ac19","execution":{"iopub.status.busy":"2024-05-10T19:08:05.875442Z","iopub.execute_input":"2024-05-10T19:08:05.876144Z","iopub.status.idle":"2024-05-10T19:08:06.215363Z","shell.execute_reply.started":"2024-05-10T19:08:05.876112Z","shell.execute_reply":"2024-05-10T19:08:06.214289Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"all_outs","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"aNuWTwdAp_8M","outputId":"2df6d08d-350a-4c71-80ad-d371e6e16883","execution":{"iopub.status.busy":"2024-05-10T19:08:07.001105Z","iopub.execute_input":"2024-05-10T19:08:07.001966Z","iopub.status.idle":"2024-05-10T19:08:07.007731Z","shell.execute_reply.started":"2024-05-10T19:08:07.001933Z","shell.execute_reply":"2024-05-10T19:08:07.006832Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"['cardiovascular', 'no', 'no', 'no', 'no', 'no', 'no']"},"metadata":{}}]},{"cell_type":"code","source":"all_answers","metadata":{"execution":{"iopub.status.busy":"2024-05-10T19:08:08.353272Z","iopub.execute_input":"2024-05-10T19:08:08.353914Z","iopub.status.idle":"2024-05-10T19:08:08.359784Z","shell.execute_reply.started":"2024-05-10T19:08:08.353885Z","shell.execute_reply":"2024-05-10T19:08:08.358797Z"},"trusted":true},"execution_count":100,"outputs":[{"execution_count":100,"output_type":"execute_result","data":{"text/plain":"['pa',\n '5.6cm focal, predominantly hypodense',\n 'hydropneumothorax',\n 'yes',\n 'axial',\n '5.6cm focal, predominantly hypodense',\n 'yes']"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}